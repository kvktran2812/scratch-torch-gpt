{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4c2bda-8454-411c-b61d-2ddc94191829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a8b242c4f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b97394-4ed3-4e21-847b-fd933f6ef2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "Number of characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Number of characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0c2b3b-983b-4cac-b2e6-43c3a2862b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f07a407-1c9c-46fb-b45e-e7afe67b1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adfcfa6-9d2d-46cc-b9ea-308bb9e25046",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = k @ q.transpose(-2, -1)\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1347616-77e8-4cdf-91be-7356e5fecdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention head computation:\n",
    "n_embd = 32\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "    \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "    \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        out = wei @ v \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd2c8be-77ce-4027-87a7-515542447c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.sa_head = Head(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb # B, T, C\n",
    "        x = self.sa_head(x) # B, T, C\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_token):\n",
    "        for _ in range(max_new_token):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a6b8c66-b0f9-4fc4-9999-ec1cb370e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.1541, val loss 4.1553\n",
      "step 100: train loss 2.8980, val loss 2.9141\n",
      "step 200: train loss 2.6617, val loss 2.6645\n",
      "step 300: train loss 2.5322, val loss 2.5353\n",
      "step 400: train loss 2.4546, val loss 2.4633\n",
      "step 500: train loss 2.4160, val loss 2.4255\n",
      "step 600: train loss 2.3902, val loss 2.4065\n",
      "step 700: train loss 2.3741, val loss 2.3912\n",
      "step 800: train loss 2.3607, val loss 2.3812\n",
      "step 900: train loss 2.3458, val loss 2.3685\n",
      "step 1000: train loss 2.3366, val loss 2.3616\n",
      "step 1100: train loss 2.3243, val loss 2.3530\n",
      "step 1200: train loss 2.3181, val loss 2.3448\n",
      "step 1300: train loss 2.3101, val loss 2.3414\n",
      "step 1400: train loss 2.3058, val loss 2.3374\n",
      "step 1500: train loss 2.2995, val loss 2.3308\n",
      "step 1600: train loss 2.2950, val loss 2.3329\n",
      "step 1700: train loss 2.2913, val loss 2.3271\n",
      "step 1800: train loss 2.2876, val loss 2.3276\n",
      "step 1900: train loss 2.2871, val loss 2.3290\n",
      "step 2000: train loss 2.2829, val loss 2.3255\n",
      "step 2100: train loss 2.2813, val loss 2.3228\n",
      "step 2200: train loss 2.2793, val loss 2.3227\n",
      "step 2300: train loss 2.2749, val loss 2.3226\n",
      "step 2400: train loss 2.2760, val loss 2.3231\n",
      "step 2500: train loss 2.2724, val loss 2.3179\n",
      "step 2600: train loss 2.2709, val loss 2.3212\n",
      "step 2700: train loss 2.2708, val loss 2.3186\n",
      "step 2800: train loss 2.2685, val loss 2.3190\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 2500\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d8c7fc-f81c-4caf-879f-e9706adf1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WANGRHDI bridcowr,\n",
      "This sowr Kibadset bobe doeravegr-ans mealilanss:\n",
      "Want he usquet vet?\n",
      "\n",
      "MEXENO:\n",
      "Aen wice my.\n",
      "\n",
      "HDYUSYom onoug\n",
      "Yowno, tof isth bot milf dill, at miree sen cie lat Het drovets, and Win ngan ilerabous lelind meall liser onchiry:\n",
      "Asprinesspll, yo wllingu normopetelaves\n",
      "Momy yu, demet akleo Winso wher eiinge wisti dourive wees ime st sot owrif thure kind thrupirf sor; igre! mef thie male onto, af Prred my om.\n",
      "\n",
      "HETY:\n",
      "E'ss,\n",
      "Sbus, wardave aces art my din cme amy aney Iry ts I fr yo voucken pand, bemary.\n",
      "\n",
      "HARIWOF RIORD oben anghse.\n",
      "\n",
      "And fout senet.\n",
      "\n",
      "Thy showne, ins win llety ome.\n",
      "\n",
      "Thuco frepy tshintchigl.\n",
      "\n",
      "Andias wetlal wave.\n",
      "\n",
      "LAWIOPRAUNTE:\n",
      "Rour imd assche, os coknovet Hose st ums histe fe'd tass:\n",
      "Whit Clof; chun hes, nd dud ton, moxcharcheanto ankes agh whein\n",
      "'As mes sleve bumlon mod the wllo no'ld id, morsed\n",
      "Formy?\n",
      "TI idurd porvenand, do thieyr ivethe of tiund the nof the sut nexch you on whandeng itth ougle Et llollke, on sothan thean, delwat do ived:\n",
      "Ther, foru;\n",
      "yo knogin.\n",
      "\n",
      "TI tane ith yorealsit fet Hen ache wher tchave, do pr tattand, and Coay hors, and\n",
      "Ase, meseveminds se me wortingin ith worvigth Whou wow ndich\n",
      "T ber be\n",
      "Hed hy sen nirstha theint de mas, the any igoet myou menat my borfe sing brwimn isell.\n",
      "\n",
      "Nor wo I dis;\n",
      "Sa det hay'sill! wheramperal lalop!\n",
      "AT:\n",
      "Tollacte ome ba def thant.\n",
      "\n",
      "KET:\n",
      "Of alil, mas, hy it ih st doubelldir donundll quevey menas bend sew ocurpne, the to thim theabae; I nounga thit nenanto me wine te you thel!\n",
      "Hefiss yee ot,\n",
      "But wallolers eterins; sis justh;\n",
      "Her pporrars, es spaith bud olks-latscece:\n",
      "O thers\n",
      "et sishe,\n",
      "Of BLOUMLIO:\n",
      "Frove my Bcy preirno derd hry.\n",
      "\n",
      "TET thin.\n",
      "\n",
      "BENNUFHARIGST:\n",
      "Toun. Pratlir arit, gigthathitel, maver, byoras o'd;\n",
      "\n",
      "Thathean hawrar hy nofen:\n",
      "Bth wiflout mef ma than:\n",
      "Her, srof tito amunge,\n",
      "Youl? Alld weat rere wincof foon s' I orre burs yo onerowur lwel he puns foourgo to wheeak.\n",
      "\n",
      "SA:\n",
      "An:\n",
      "Pannga woury porut,\n",
      "The,\n",
      "I LOLent,\n",
      "Jou. It Vence stho, Jures woat noll briin shown's fith thee ss me otenathe won ou\n",
      "Maru\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_token=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d7b92-180f-48d5-80e3-76c483544545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
