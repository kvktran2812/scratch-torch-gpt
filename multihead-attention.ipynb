{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b02aff-5a23-42d9-81c2-464864e97d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b9ec42c4d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecf17e5-9833-4873-9a6c-cf2e1956cc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65\n",
      "Number of characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Number of characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34c4bac-0ebf-4b0e-8238-01cee6ac34a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5678c80-3db7-45b5-b49f-3a0d699c69da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30baab5f-ef7e-491a-bf2e-5fd6ec3774df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b043c1a-d413-4869-982e-72638fddd40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "    \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "    \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.multihead = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.multihead], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, n_embd):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHead(n_heads, n_embd//n_heads)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.multihead(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded906f4-085d-4d50-8257-aa7034d62ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.positional_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_head, n_embd) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.positional_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb # B, T, C\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # B, T, vocab_size\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_token):\n",
    "        for _ in range(max_new_token):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2471db1-6a3d-43fb-8676-aab73129788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2849, val loss 4.2823\n",
      "step 500: train loss 2.0113, val loss 2.0964\n",
      "step 1000: train loss 1.5982, val loss 1.7789\n",
      "step 1500: train loss 1.4405, val loss 1.6401\n",
      "step 2000: train loss 1.3434, val loss 1.5685\n",
      "step 2500: train loss 1.2801, val loss 1.5275\n",
      "step 3000: train loss 1.2291, val loss 1.5077\n",
      "step 3500: train loss 1.1847, val loss 1.4871\n",
      "step 4000: train loss 1.1488, val loss 1.4857\n",
      "step 4500: train loss 1.1107, val loss 1.4783\n",
      "step 4999: train loss 1.0774, val loss 1.4856\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb29acc-33bc-4d8c-827e-88940bdf1bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BRUTUS:\n",
      "Quick, nobles;\n",
      "I have been Compey; who comes you\n",
      "As true darling.\n",
      "\n",
      "CORIOLANUS:\n",
      "I hope, sir:\n",
      "Veenin!\n",
      "\n",
      "SICINIUS:\n",
      "You stand with neither dear: forget them not\n",
      "That sogs about to bed her must, and\n",
      "May shall learn up him, but we come in Rome.\n",
      "\n",
      "SICIUS:\n",
      "Been he well;\n",
      "To strike so: most fitteen for presence parts him, and\n",
      "The gentleman herein peon, and the wan,\n",
      "Act of walls; and then for two year is a father? Thy\n",
      "wounds a Nothoma.\n",
      "\n",
      "First Musician:\n",
      "Now, Romeo, take him To God's outher break.\n",
      "Trembland, take thy opposeths yoks! that I had\n",
      "full one couples to this grave naved subjects. What, thou\n",
      "Thou true mayst not stolen.\n",
      "\n",
      "Second Lord:\n",
      "Not sight, but that I do burd to bed, our livers,\n",
      "Such the curtains of being lost, and not destructed\n",
      "From from the children.\n",
      "\n",
      "Clown:\n",
      "Clarence! thou dost, for on the worst that all\n",
      "Affraitful Romand in prince fasthers may.\n",
      "\n",
      "PRINCE Servingman:\n",
      "What she law, when he liverns his tenter shall kill be.\n",
      "Alas it is a travitory, she--a\n",
      "door, this strength for i' the man to see the shreeky\n",
      "both.\n",
      "\n",
      "HASTINGS:\n",
      "Were as the day she did, in my third, I realth\n",
      "The boldness in confesses of your highness. Blords,\n",
      "My noble father was a word's bushing bawt: by\n",
      "Seems the king what steps o' merrily, but a law,\n",
      "Your father increase: not she is warries: how my tale!\n",
      "\n",
      "JULIET:\n",
      "Yes, sitten here's.\n",
      "\n",
      "ESCALLA:\n",
      "Why, no more three boy. 'Fare you ballad!\n",
      "\n",
      "AUDIUS:\n",
      "Tell holy met me be an a such, you shall\n",
      "As you bear here may on my part cry.\n",
      "Therefore, sir, I know no good to of dear them;\n",
      "Unless should be well and one: and so you got?\n",
      "\n",
      "EDhre:\n",
      "You do remain?\n",
      "\n",
      "VOLUMNIA:\n",
      "Fly, or now now my good sir, your highness:\n",
      "I dare not my good princely wife at who return,\n",
      "To whent you for some that encounter'd,\n",
      "By note fright commands threw twice Tybalt,\n",
      "Had set you and bound from my left a gage\n",
      "Had; and spent therefore be, sweet prince,\n",
      "Who hath sagges him afraid His crownshion;\n",
      "To want him that give us.\n",
      "\n",
      "LADY CAPULET:\n",
      "Villain, I do beseech you deny!\n",
      "For she comes against them my be\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_token=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab09b3a-83d0-4166-8126-102eec5a7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"checkpoints/model_v1.pth\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6d719-5cc1-4ff1-b260-e5344859d4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
