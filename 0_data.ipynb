{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72439a9a-dbe3-43d2-b6ec-f0f4c5b916ab",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "In this note book, we will explore how the pre training phase, before we train the GPT model. \n",
    "\n",
    "# A few words before we start\n",
    "This whole project will demonstrate how to train a GPT model architecture from scratch but we will just create a proof of concept project. Real GPT were trained on billions of tokens (Terrabytes of data with thousands of GPUs) to achieve the current GPT. Our project will just train on around a hundred thousands tokens with our model is around several millions parameters (quite small), and the training will take 10 to 30 mins depends on your machine computational resources. You will see at the end, although the model can't create meaningful sentence but it manage to create Vietnamese words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cc5a47-3b87-462a-b058-2a55f9829875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebb7e1-1d68-4dbd-986b-7cf3f6acab5c",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "First of all we will need to prepare the vocab size of the Vietnamese language. Technically in here we will just parse through Truyen Kieu dataset and count for the unique characters in the text. In real life scenario, of course we will have a dictionary to know the correct words and we have to work on unexpected new words as well for example new unique name of certain things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31c332f-46a2-4ad1-9db4-62ac61a602d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 129\n",
      "Number of characters: 104804\n"
     ]
    }
   ],
   "source": [
    "with open('data/truyen_kieu_clean.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Number of characters:\", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2265da-0ef8-4f7b-b4cd-926a08078ada",
   "metadata": {},
   "source": [
    "## Encoder and decoder\n",
    "Decoder and encoder will be based on your tokenizer, how you will transform from text to numbers. In correct GPT paper, the tokenizer is sub-word, which mean it predict part of a word then glue them together to become words and sentences. In here since we only have really small amount of text, we will use character-by-character tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5658ffda-74e3-45b3-ae7c-229fd9dbbf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 26, 47, 74, 42, 1, 43, 74, 42, 1, 49, 47, 44, 43, 37, 1, 34, 69, 39, 1, 43, 37, 82, 113, 39, 1, 49, 32, 4, 0, 12, 38, 122, 1, 49, 57, 39, 1, 34, 38, 122, 1, 42, 102, 43, 38, 1, 40, 38, 62, 44, 1, 41, 57, 1, 37, 38, 62, 49, 1, 43, 38, 32, 50, 6, 0, 26, 47, 84, 39, 1, 46, 50, 32, 1, 42, 111, 49, 1, 34, 50, 111, 34, 1, 33, 100, 1, 35, 59, 50, 4, 0, 20, 38, 122, 43, 37, 1, 76, 39, 99, 50, 1, 49, 47, 68, 43, 37, 1, 49, 38, 85, 53, 1, 42, 57, 1, 76, 32, 50, 1, 76, 112, 43, 1, 41, 66, 43, 37, 6, 0]\n",
      "\n",
      "Trăm năm trong cõi người ta,\n",
      "Chữ tài chữ mệnh khéo là ghét nhau.\n",
      "Trải qua một cuộc bể dâu,\n",
      "Những điều trông thấy mà đau đớn lòng.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encoder = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decoder = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "test_text = \"\"\"\n",
    "Trăm năm trong cõi người ta,\n",
    "Chữ tài chữ mệnh khéo là ghét nhau.\n",
    "Trải qua một cuộc bể dâu,\n",
    "Những điều trông thấy mà đau đớn lòng.\n",
    "\"\"\"\n",
    "print(encoder(test_text))\n",
    "print(decoder(encoder(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12322c4-a585-40b9-8b0f-6813dc90a2cf",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In here, we split the data to train and eval data of ratio 0.9. We will train 90% of the data and eval the model on the other 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4c152f-3c1f-4885-a95a-a532e5ea683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260dc67-abc0-487d-a841-c98541c29c82",
   "metadata": {},
   "source": [
    "## Batch\n",
    "\n",
    "In the below code is the function to get batch each time we train the model. One special thing about GPT training is: it use steps instead of epochs to train the model, unlike other common application. The reason for this is for efficiency, in text data loading billions of tokens can be expensive, instead we randomly select some chunks of data here and there and train on those chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02094e7-6725-4e8b-8e9d-2fc8ac281e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([64, 32])\n",
      "y shape: torch.Size([64, 32])\n",
      "x: tensor([[39, 98, 49,  ..., 43, 37,  1],\n",
      "        [ 1, 43, 38,  ..., 38, 50,  1],\n",
      "        [76, 98, 43,  ..., 50, 68, 43],\n",
      "        ...,\n",
      "        [ 1, 76, 60,  ..., 71, 32,  1],\n",
      "        [ 6,  0, 18,  ..., 38, 64, 43],\n",
      "        [53,  1, 48,  ..., 76, 57, 44]])\n",
      "y: tensor([[98, 49,  1,  ..., 37,  1, 48],\n",
      "        [43, 38, 58,  ..., 50,  1, 34],\n",
      "        [98, 43,  1,  ..., 68, 43,  1],\n",
      "        ...,\n",
      "        [76, 60,  1,  ..., 32,  1, 47],\n",
      "        [ 0, 18, 57,  ..., 64, 43,  1],\n",
      "        [ 1, 48, 32,  ..., 57, 44,  4]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size:int = 64, block_size: int = 32):\n",
    "    idx = torch.randint(len(train_data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([train_data[i:i+block_size] for i in idx])\n",
    "    y = torch.stack([train_data[i+1:i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(train_data)\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e845fa-e713-4403-a8ed-5fc4cb627bab",
   "metadata": {},
   "source": [
    "## Flash Attention (Optional)\n",
    "\n",
    "These code below are optional if you want to use Flash Attention instead of normal Attention in your training. The Flash Attention will calculate the attention values with 2x to 4x faster then traditional approach but it's optional. Without Flash Attention, you can still train your model fine without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081b5353-f93e-4a9e-a8c4-18db4abad972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "2.5.1+cu121\n",
      "12.1\n",
      "90100\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "print(torch.backends.cuda.matmul.allow_tf32)  # Should print True\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n",
    "print(torch.__version__)  # Check your PyTorch version\n",
    "print(torch.version.cuda)  # Check the CUDA version PyTorch was built with\n",
    "print(torch.backends.cudnn.version())  # Check the cuDNN version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e300f-cd45-45b9-8abd-6a111e03ea95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
